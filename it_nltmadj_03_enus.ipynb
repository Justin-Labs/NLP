{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Labs/NLP/blob/main/it_nltmadj_03_enus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EIoSNLUUBoO"
      },
      "source": [
        "# Navigating Basic Polyglot Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL1U3CWrKptr"
      },
      "source": [
        "# it_nltmadj_03_enus_04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ycVt3vbTlCj"
      },
      "source": [
        "- Exploring polyglot features\n",
        " - Multi-language tokenization\n",
        " - Multi-language Named Entity Recognition\n",
        " - Multi-language Part-of-speech Tagging\n",
        " - Multi-Language Morphological Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9cvzc7DLIfX"
      },
      "source": [
        "## Multi-Language Tokenization\n",
        "\n",
        "The technique of identifying the text boundaries of words and sentences is known as tokenization. We can identify sentence boundaries first, then tokenize each sentence to identify the terms that comprise the sentence. Of course, we may tokenize the words first, then partition the token sequence into sentences.\n",
        "\n",
        "%pip install polyglot #Ployglot is a natural language pipeline that supports massive multilingual applications.\n",
        "%pip install PyICU #PyICU is a python extension implemented in C++ that wraps the C/C++ ICU library. It is known to\n",
        "# also work as a PyPy extension. Where the ICU stands for \"International Components for Unicode\"\n",
        "%pip install pycld2 # Python bindings for the Compact langauge Detect 2 (CLD2).\n",
        "%pip install Morfessor #Morfessor is a tool for unsupervised and semi-supervised morphological segmentation.\n",
        "%pip install polyglot transliteration # Transliteration is the conversion of a text from one script to another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML9LPVqCLx8h"
      },
      "outputs": [],
      "source": [
        "%pip install polyglot\n",
        "%pip install PyICU\n",
        "%pip install pycld2\n",
        "%pip install Morfessor\n",
        "%pip install polyglot transliteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJg5kzdtJ4B9"
      },
      "outputs": [],
      "source": [
        "import polyglot\n",
        "from polyglot.text import Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb-bCiI9Mam8"
      },
      "source": [
        "Word Tokenization\n",
        "To invoke our word tokenizer, we must first create a Text object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cey13tlLgOr"
      },
      "outputs": [],
      "source": [
        "blob = u\"\"\"\n",
        "机器学习 (ML) 是对可以通过经验和数据使用自动改进的计算机算法的研究。它被视为人工智能的一部分。机器学习算法基于样本数据（称为训练数据）构建模型，\n",
        "以便在没有明确编程的情况下做出预测或决策。 机器学习算法广泛用于各种应用，例如医学、电子邮件过滤、语音识别和计算机视觉，在这些领域开发传统算法来执行所需的任务是困难的或不可行的。\n",
        "\"\"\"\n",
        "text = Text(blob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVkrE_slZ2v9"
      },
      "outputs": [],
      "source": [
        "text.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P4izRXyaGIf"
      },
      "source": [
        "Since ICU boundary break algorithms are language aware, polyglot will detect the language used first before calling the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFuW7PD-Z9aE"
      },
      "outputs": [],
      "source": [
        "print(text.language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLlbFfDfbzGX"
      },
      "source": [
        "## Multi-Language Named Entity Extraction\n",
        "\n",
        "The task known as entity extraction seeks to extract phrases from the plain text that correspond to entities. Polyglot distinguishes three types of entities:\n",
        "\n",
        "- Cities, countries, regions, continents, neighborhoods, administrative divisions... are all examples of locations (Tag: I-LOC).\n",
        "- Organizations (Tag: I-ORG): sports teams, newspapers, banks, universities, schools, non-profit organizations, businesses, and so on.\n",
        "- Individuals (Tag: I-PER): politicians, scientists, artists, athletes, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGD_bgzKdUEb"
      },
      "source": [
        "### Languages Coverage\n",
        "\n",
        "The models were trained using datasets automatically scraped from Wikipedia. Polyglot is presently available in 40 main languages. We can query our do\n",
        "wnload manager for which tasks are supported by polyglot, as the following:\n",
        "downloader.supported_tasks(lang='en\")\n",
        "[u'embeddings2',\n",
        "u'counts2',\n",
        "u'pos2',\n",
        "u'ner2',\n",
        "u'sentiment2',\n",
        "u'morph2',\n",
        "u'tsne2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWblFpzrbe1v"
      },
      "outputs": [],
      "source": [
        "from polyglot.downloader import downloader\n",
        "print(downloader.supported_languages_table(\"ner2\", 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3GtH5Axdk8R"
      },
      "outputs": [],
      "source": [
        "# Dowloading models\n",
        "\n",
        "%%bash\n",
        "polyglot download embeddings2.en ner2.en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHRbQpklgf8e"
      },
      "source": [
        "Let's take a look into the example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPtCHga0dvjY"
      },
      "outputs": [],
      "source": [
        "blob = \"\"\"US President Joe Biden and Russian President Vladimir Putin may meet in early 2022.\"\"\"\n",
        "text = Text(blob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rx61Wy3hA6D"
      },
      "source": [
        "Let's query all entities mentioned in the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFJoFKQ7hJ9F"
      },
      "outputs": [],
      "source": [
        "text.entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIB6G3pBhKuB"
      },
      "outputs": [],
      "source": [
        "for sent in text.sentences:\n",
        "  print(sent, \"\\n\")\n",
        "  for entity in sent.entities:\n",
        "    print(entity.tag, entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEB-ZhNNieOu"
      },
      "outputs": [],
      "source": [
        "# We can do careful inspection of the First entity Joe Biden, we can locate the position of the entity within the sentence.\n",
        "joe = sent.entities[0]\n",
        "sent.words[joe.start: joe.end]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60rPXOXq9Vm"
      },
      "source": [
        "# Multi-Language Part of Speech Tagging\n",
        "\n",
        "The goal of the part of speech tagging challenge is to give a category to each word/token in plain text that indicates the syntactic functionality of the word occurrence.\n",
        "\n",
        "Polyglot recognises 17 parts of speech, which are referred to as the universal part. of speech tag set:\n",
        "\n",
        "- ADJ: adjective\n",
        "- ADP: adposition\n",
        "- ADV: adverb\n",
        "- AUX: auxiliary verb\n",
        "- CONJ: coordinating conjunction\n",
        "- DET: determiner\n",
        "- INTJ: interjection\n",
        "- NOUN: noun\n",
        "- NUM: numeral\n",
        "- PART: particle\n",
        "- PRON: pronoun\n",
        "- PROPN: proper noun\n",
        "- PUNCT: punctuation\n",
        "- SCONJ: subordinating conjunction\n",
        "- SYM: symbol\n",
        "- VERB: verb\n",
        "- X: other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJL8NFhiuQ4c"
      },
      "outputs": [],
      "source": [
        "# Here we will look, How many language Polyglot POS support\n",
        "from polyglot.downloader import downloader\n",
        "print(downloader.supported_languages_table(\"pos2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ZMjhEwuanK"
      },
      "outputs": [],
      "source": [
        "# Downloading necessary english model of POS from polyglot\n",
        "%%bash\n",
        "polyglot download embeddings2.en pos2.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKQRuAbCuHFL"
      },
      "outputs": [],
      "source": [
        "from polyglot.text import Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlBCP36Rk9lY"
      },
      "outputs": [],
      "source": [
        "blob = '''Biden and Putin may meet in early 2022. If that sounds like deja vu, you’re right.\n",
        "After Russia mobilized troops on Ukraine’s border last April, a Biden–Putin summit took place in mid-June in Geneva.\n",
        "Long ago, North Korea discovered that missile launches were an effective way of getting Washington’s attention'''\n",
        "text = Text(blob)\n",
        "text.pos_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyvveKcvuAal"
      },
      "outputs": [],
      "source": [
        "# After calling the pos_tags property once, the words objects will carry the POS tags.\n",
        "text.words[0].pos_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dEJ60U9vHXs"
      },
      "source": [
        "# Morphological Analysis\n",
        "\n",
        "Polyglot provides trained morfessor models for generating morphemes from words. The Morpho project aims to provide unsupervised data-driven approaches for discovering the regularities underpinning word formation in natural languages. The Morpho project is particularly interested in the finding of morphemes, which are the primordial units of syntax, the smallest independently meaningful items in a language's utterances. Morphemes are significant in automatic language creation and recognition, especially in languages where words might have many distinct inflected forms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avBZ7hvsvBtM"
      },
      "outputs": [],
      "source": [
        "# Morphemes language coverage\n",
        "\n",
        "from polyglot.downloader import downloader\n",
        "print(downloader.supported_languages_table(\"morph2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK8HBYxB0y3e"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "polyglot download morph2.en morph2.ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVDsflKX07VN"
      },
      "outputs": [],
      "source": [
        "from polyglot.text import Text, Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NB9ip4c0-WA"
      },
      "outputs": [],
      "source": [
        "# Word Segmentation\n",
        "\n",
        "\n",
        "words = [\"preprocessing\", \"processor\", \"invaluable\", \"thankful\", \"crossed\"]\n",
        "for w in words:\n",
        "  w = Word(w, language=\"en\")\n",
        "  print(\"{:<20}{}\".format(w, w.morphemes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OF5TpEY1HKt"
      },
      "outputs": [],
      "source": [
        "# Sentence Segmentation\n",
        "\n",
        "blob = \"Wewillmeettoday.\"\n",
        "text = Text(blob)\n",
        "text.language = \"en\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va1oSPSU1KK1"
      },
      "outputs": [],
      "source": [
        "text.morphemes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDe0H4Ce1ZC9"
      },
      "source": [
        "Section Ends Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0v0WxTx1XYT"
      },
      "source": [
        "# it_nltmadj_03_enus_05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP0fHslj1i3Y"
      },
      "source": [
        "Exploring Polyglot features\n",
        " - Language Detection\n",
        " - Multi-lingual Sentiment Analysis\n",
        " - Tranliteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjA79-bF17-3"
      },
      "source": [
        "## Language Detection\n",
        "\n",
        "Polyglot is reliant on the `pycld2` library, which is reliant on the cld2 library, to detect the language(s) used in plain text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4yBIt0H1ilK"
      },
      "outputs": [],
      "source": [
        "from polyglot.detect import Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA7xcU_v1YiR"
      },
      "outputs": [],
      "source": [
        "arabic_text = u'''يحتفل الناس حول العالم بأعياد الميلاد، وهو واحد من أقدس الأوقات في التقويم المسيحي.\n",
        "\n",
        "مع هذا، وللعام الثاني على التوالي، يشارك أعداد أقل في المراسم الكنسية والفعاليات الأخرى بسبب استمرار تفشي جائحة كورونا.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESm83gUz2Owx"
      },
      "outputs": [],
      "source": [
        "detector = Detector(arabic_text)\n",
        "print(detector.language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSrOd8qq2R0A"
      },
      "outputs": [],
      "source": [
        "# mixed text\n",
        "\n",
        "mixed_text = u\"\"\"\n",
        "China (simplified Chinese: 中国; traditional Chinese: 中國),\n",
        "officially the People's Republic of China (PRC), is a sovereign state located in East Asia.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHu7DpE53GAv"
      },
      "source": [
        "If the text contains snippets from different languages, the detector is able to find the most probable langauges used in the text. For each language, we can query the model confidence level:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnCv1EL23Gw0"
      },
      "outputs": [],
      "source": [
        "for language in Detector(mixed_text).languages:\n",
        "  print(language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-9UjBgY3L6j"
      },
      "outputs": [],
      "source": [
        "for line in mixed_text.strip().splitlines():\n",
        "  print(line + u\"\\n\")\n",
        "  for language in Detector(line).languages:\n",
        "    print(language)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slFTk7zX3X2U"
      },
      "outputs": [],
      "source": [
        "# Supported languages\n",
        "\n",
        "from polyglot.utils import pretty_list\n",
        "print(pretty_list(Detector.supported_languages()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "socNIFV53eyd"
      },
      "source": [
        "## Multi-Language Sentiment Analysis\n",
        "\n",
        "For 136 languages, Polyglot contains polarity lexicons. The polarity of the words was measured on a three-degree scale: +1 for positive words and -1 for negative phrases. Words that are neutral will receive a score of 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZKfAU6u3boL"
      },
      "outputs": [],
      "source": [
        "# Language Coverage\n",
        "\n",
        "from polyglot.downloader import downloader\n",
        "print(downloader.supported_languages_table(\"sentiment2\", 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e684dT3P4jNz"
      },
      "outputs": [],
      "source": [
        "# Dowloading models\n",
        "\n",
        "%%bash\n",
        "polyglot download sentiment2.en sentiment2.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok245YdF3vbW"
      },
      "outputs": [],
      "source": [
        "# Polarity - To investigate a word's polarity, we may simply call its own attribute polarity.\n",
        "\n",
        "text = Text(\"The movie named Avengers was really good to watch. It's a big hit !\")\n",
        "\n",
        "print(\"{:<16}{}\".format(\"Word\", \"Polarity\")+\"\\n\"+\"-\"*30)\n",
        "for w in text.words:\n",
        "    print(\"{:<16}{:>2}\".format(w, w.polarity))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwkbvKvO4Q6i"
      },
      "outputs": [],
      "source": [
        "# Entity Sentiment - We may generate a more specific sentiment score for an entity mentioned in text as follows:\n",
        "\n",
        "blob = (\"Barack Obama gave a fantastic speech last night. \"\n",
        "        \"Reports indicate he will move next to New Hampshire.\")\n",
        "text = Text(blob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnwSUWyb5VEQ"
      },
      "outputs": [],
      "source": [
        "# First, we must divide the text into sentences, which will limit the words that alter an entity's attitude to those specified in the sentence.\n",
        "\n",
        "first_sentence = text.sentences[0]\n",
        "print(first_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9PsbZTo5fm-"
      },
      "outputs": [],
      "source": [
        "# We will extract the entities\n",
        "\n",
        "first_entity = first_sentence.entities[0]\n",
        "print(first_entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIzp5Jde5ioZ"
      },
      "outputs": [],
      "source": [
        "# Finally, for each thing we discovered, we may compute the strength of its positive or negative emotion on a scale of 0 to 1.\n",
        "\n",
        "print('Positive Sentiment=>',first_entity.positive_sentiment)\n",
        "print('Negative Sentiment=>',first_entity.negative_sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO9ovQge6WcI"
      },
      "source": [
        "## Transliteration\n",
        "\n",
        "Transliteration is the process of converting a text from one script to another. For example, \"Ellēnikḗ Dēmokratía\" is a Latin transcription of the Greek term \"Ελληνική Δημοκρατία\" which is commonly interpreted as \"Hellenic Republic.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_17wdUMK5_Lr"
      },
      "outputs": [],
      "source": [
        "from polyglot.transliteration import Transliterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eMD5YpE652t"
      },
      "outputs": [],
      "source": [
        "# Language Coverage\n",
        "\n",
        "from polyglot.downloader import downloader\n",
        "print(downloader.supported_languages_table(\"transliteration2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g5WloCT68No"
      },
      "outputs": [],
      "source": [
        "# Downloading required model\n",
        "\n",
        "%%bash\n",
        "polyglot download embeddings2.en transliteration2.ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFhb1Pcb7Blw"
      },
      "outputs": [],
      "source": [
        "from polyglot.text import Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NyisZez7Eca"
      },
      "outputs": [],
      "source": [
        "blob = \"\"\"Lufthansa plans to cut33,000 flights from its winter schedule due to the spread of the\n",
        "Omicron coronavirus variant and related travel restrictions, CEO Carsten Spohr told the Frankfurter\n",
        "Allgemeine Sonntagszeitung newspaper.\"\"\"\n",
        "text = Text(blob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwT-bCF37NWu"
      },
      "outputs": [],
      "source": [
        "for x in text.transliterate(\"ar\"):\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuGt6gox7cD-"
      },
      "source": [
        "Section Ends here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQu7uRHb7WWf"
      },
      "source": [
        "# it_nltmadj_03_enus_06\n",
        "# Basic TextBlob Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq2o92PsPMKS"
      },
      "source": [
        "### Exploring features of textBlob\n",
        " - Installation\n",
        " - Noun Phrase Extraction\n",
        " - Part-of-speech\n",
        " - Parsing\n",
        " - WordNet Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An7hWoPuProi"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAISf7Zl7bL6"
      },
      "outputs": [],
      "source": [
        "!pip install -U textblob\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJNzULuHPvDC"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEcn_bnEP7jv"
      },
      "source": [
        "## Noun-Phrase Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HehtuL2pQDJp"
      },
      "outputs": [],
      "source": [
        "tb=TextBlob('''Lufthansa plans to cut33,000 flights from its winter schedule due to the spread of\n",
        "the Omicron coronavirus variant and related travel restrictions, CEO Carsten Spohr told the Frankfurter Allgemeine Sonntagszeitung newspaper.''')\n",
        "tb.noun_phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8pXDeERZHW"
      },
      "source": [
        "### Part-of-speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNTJQgSdRbg0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9AUMhqTWhvI"
      },
      "outputs": [],
      "source": [
        "tb1=TextBlob(\"Biden and Putin may meet in early 2022. If that sounds like deja vu, you’re right\")\n",
        "tb1.tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBgvB5QtQoy1"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJTHLWTiQZmt"
      },
      "outputs": [],
      "source": [
        "from textblob.parsers import PatternParser\n",
        "tb2 = TextBlob('''Lufthansa plans to cut33,000 flights from its winter schedule due to the spread of\n",
        "the Omicron coronavirus variant and related travel restrictions, CEO Carsten Spohr told the Frankfurter Allgemeine Sonntagszeitung newspaper.''', parser=PatternParser())\n",
        "tb2.parse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HivrVs3mXtGa"
      },
      "source": [
        "### WordNet Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sGJCDBZYCvP"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_di5q4qW9zX"
      },
      "outputs": [],
      "source": [
        "# You may retrieve a Word's synsets using the synsets property or the get_synsets method, optionally passing in a chunk of speech.\n",
        "from textblob import Word\n",
        "from textblob.wordnet import VERB\n",
        "word = Word(\"octopus\")\n",
        "print(word.synsets)\n",
        "print('---------//----------------//----------------------//---------------')\n",
        "print(Word(\"hack\").get_synsets(pos=VERB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxTU5DCPX_XV"
      },
      "outputs": [],
      "source": [
        "from textblob.wordnet import Synset\n",
        "octopus = Synset('octopus.n.02')\n",
        "shrimp = Synset('shrimp.n.03')\n",
        "octopus.path_similarity(shrimp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AqpvG_gYi9r"
      },
      "source": [
        "# it_nltmadj_03_enus_07\n",
        "# Advanced TextBlob Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_T3QAKNY4xE"
      },
      "source": [
        "## Exploring features of TextBlob\n",
        " - Sentiment Analysis\n",
        " - Classification model\n",
        " - Tokenization\n",
        " - Word/Phrases Frequencies\n",
        " - Word Inflcition\n",
        " - Spell Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKfhkUrgdAcw"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "Sentiment is the namedtuple returned by the sentiment property (polarity, subjectivity). The polarity score is a number between -1.0 and 1.0. Subjectivity is a float between 0.0 and 1.0, with 0.0 being extremely objective and 1.0 being extremely subjective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GBHTAWIYRqe"
      },
      "outputs": [],
      "source": [
        "tb3 = TextBlob(\"The movie named Avengers was really good to watch. It's a big hit !, What great fun!\")\n",
        "print('polarity and subjectivity =>',tb3.sentiment)\n",
        "print('polarity =>',tb3.sentiment.polarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnTRkKBjxmSD"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yDGX4HCdypY"
      },
      "outputs": [],
      "source": [
        "# TextBlobs may be broken down into words or phrases.\n",
        "tb4 = TextBlob(\"Beautiful is better than ugly. \"\n",
        "              \"Explicit is better than implicit. \"\n",
        "           \"Simple is better than complex.\")\n",
        "print('words=>',tb4.words)\n",
        "print('--------------------//---------------------//--------------------------//---------------------------//-----------------------//-----------')\n",
        "print('sentences=>',tb4.sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxUuNp6yeSN"
      },
      "source": [
        "## Word/Phrase frequencies\n",
        "\n",
        "In a TextBlob, there are two methods to obtain the frequency of a word or noun phrase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La7WQJPdyHuo"
      },
      "outputs": [],
      "source": [
        "# The first is to use the dictionary word counts.\n",
        "\n",
        "tb5 = TextBlob('''Lufthansa plans to cut33,000 flights from its winter schedule due to the spread of\n",
        "the Omicron coronavirus variant and related travel restrictions according to the newspaper, CEO Carsten Spohr told the Frankfurter Allgemeine Sonntagszeitung newspaper.''')\n",
        "tb5.word_counts['newspaper']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMpUWnVK2ZDy"
      },
      "outputs": [],
      "source": [
        "# The count() technique is the second option.\n",
        "tb5.words.count('newspaper')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltJK95qPNz_9"
      },
      "source": [
        "## Words Inflection\n",
        "\n",
        "Each word or sentence in TextBlob.words. words is a Word object (a Unicode subclass) having handy methods, such as inflection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FWARnWNNmSO"
      },
      "outputs": [],
      "source": [
        "tb6 = TextBlob('''Lufthansa plans to cut33,000 flights from its winter schedule due to the spread of\n",
        "the Omicron coronavirus variant and related travel restrictions according to the newspaper, CEO Carsten Spohr told the Frankfurter Allgemeine Sonntagszeitung newspaper.''')\n",
        "print('words=>',tb6.words)\n",
        "\n",
        "print('singularize=>',tb6.words[2].singularize())\n",
        "\n",
        "print('pluralize=>',tb6.words[-1].pluralize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waX1T-kDPZCi"
      },
      "source": [
        "## Spell Correction\n",
        "\n",
        "To make an effort at spelling correction, use the correct() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETl1CPb6O091"
      },
      "outputs": [],
      "source": [
        "tb7 = TextBlob(\"I am writing ths to let you know,you havv goood speling!\")\n",
        "print(tb7.correct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O4cW62ZP9JB"
      },
      "outputs": [],
      "source": [
        "# The spellcheck() method on Word objects produces a list of (word, confidence) tuples with spelling recommendations in the form of a list of (word, confidence) tuples.\n",
        "\n",
        "from textblob import Word\n",
        "w = Word('falibility')\n",
        "w.spellcheck()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEHPLRw8QNhC"
      },
      "source": [
        "The pattern library's spelling correction is based on Peter Norvig's \"How to Write a Spelling Corrector\". It is around 70% correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5MTbnFAQUw0"
      },
      "source": [
        "Section Ends here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfHwPe3tQXT5"
      },
      "source": [
        "# it_nltmadj_03_enus_08\n",
        "## Navigating Basic Gensim Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv84MrU8QY4a"
      },
      "source": [
        "### Exploring Features of Gensim\n",
        " - Installation\n",
        " - Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SEqOdEBQ_KJ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kQo8cVAQEmD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu6jPhV9RICS"
      },
      "outputs": [],
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "# Run in terminal or command prompt\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cik-kpQaRwp_"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pxcaZ8ESGVA"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27RTTRWTRoDt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j57I12RISd7R"
      },
      "outputs": [],
      "source": [
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D377BQVShfC"
      },
      "outputs": [],
      "source": [
        "# Import Dataset\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.target_names.unique())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWayngvrSmlQ"
      },
      "outputs": [],
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "pprint(data[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76n2VnuXSp3O"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGc4WwsOSsrF"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PasWQ9btS343"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_wAchWuS7PX"
      },
      "outputs": [],
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qMwabGNS-Yj"
      },
      "outputs": [],
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv00FjqqTCPB"
      },
      "outputs": [],
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgdStQ9bTFri"
      },
      "outputs": [],
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2_7TJd5TI1U"
      },
      "outputs": [],
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGhCg71HTNJK"
      },
      "outputs": [],
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyzinjNNTPny"
      },
      "outputs": [],
      "source": [
        "# Visualize the topics\n",
        "%pip install pandas -U\n",
        "import pandas\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeLrS6CXVNT0"
      },
      "source": [
        "Section Ends here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AGDcYSXVPyu"
      },
      "source": [
        "# it_nltmadj_03_enus_09\n",
        "## Navigating Advanced Gensim Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHrwJYOKVVnR"
      },
      "source": [
        "Exploring features of Gensim\n",
        " - Query Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcN720boVO-O"
      },
      "outputs": [],
      "source": [
        "# Shows how to search a corpus for texts that are similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrXGNTDdWaLJ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vubhrduTWgfn"
      },
      "outputs": [],
      "source": [
        "# To begin, we must first construct a corpus with which to operate. This is the same procedure as in the last instruction; if you've already finished it, you may go on to the next part.\n",
        "from collections import defaultdict\n",
        "from gensim import corpora\n",
        "\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn6GW2weWk1I"
      },
      "outputs": [],
      "source": [
        "# remove common words and tokenize\n",
        "stoplist = set('for a of the and to in'.split())\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBhh3p5nW27s"
      },
      "source": [
        "Similarity interface\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1jOV-k6W7f0"
      },
      "outputs": [],
      "source": [
        "from gensim import models\n",
        "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNkm04pbXAWU"
      },
      "source": [
        "Assume a user enters \"Human computer interaction\" into the search box. We'd want to rank our nine corpus documents by relevancy to this query in decreasing order. Unlike current search engines, we just look at one element of probable similarities here: the apparent semantic similarity of respective sentences (words). There are no backlinks, no static rankings based on a random walk, only a semantic expansion of the boolean keyword match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ltRwM-cW_Y2"
      },
      "outputs": [],
      "source": [
        "doc = \"Human computer interaction\"\n",
        "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
        "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
        "print(vec_lsi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3qNSfcCXReF"
      },
      "source": [
        "To prepare for similarity searches, we must first input all of the documents that we wish to compare to the results of following queries. They are the same nine documents that were used to train LSI, but in 2-D LSA space. But that's just a coincidence; we may be indexing a completely different corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkxXQuBSXSSj"
      },
      "outputs": [],
      "source": [
        "from gensim import similarities\n",
        "index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URkkzsjdXbFn"
      },
      "source": [
        "Persistence of indexes is addressed using the standard: func:save as well as: func:load contains the following functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCXsCHTbXfZU"
      },
      "outputs": [],
      "source": [
        "index.save('/tmp/deerwester.index')\n",
        "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5I-b0LXnwa"
      },
      "source": [
        "To compare our query document to the nine indexed documents, perform the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AusYr6zXojD"
      },
      "outputs": [],
      "source": [
        "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
        "print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g44412nFXy4v"
      },
      "source": [
        "The cosine measure yields similarities in the range -1, 1> (the greater the similarity, the higher the score), therefore the first document gets a score of 0.99809301, and so on.\n",
        "\n",
        "We sort these commonalities into descending order using ordinary Python wizardry to get the final response to the question \"Human computer interaction\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msl87QB9X0HM"
      },
      "outputs": [],
      "source": [
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "for doc_position, doc_score in sims:\n",
        "    print(doc_score, documents[doc_position])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwIk4mVcX-8f"
      },
      "source": [
        "A conventional boolean fulltext search would never produce documents 2 (\"The EPS user interface management system\") and 4 (\"Relation of user perceived reaction time to error assessment\") since they do not have any common terms with \"Human computer interaction.\" However, after using LSI, we can see that they both have high similarity scores (no. 2 is actually the most similar! ), which matches our intuition of both having a \"computer-human\" connected issue in common with the inquiry. In fact, it is because of this semantic generalisation that we use transformations and topic modelling in the first place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG8APv5XYEH-"
      },
      "source": [
        "Section Ends here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SiMA1lbYGp4"
      },
      "source": [
        "# it_nltmadj_03_enus_10\n",
        "\n",
        "## Core NLP Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEn6dUBUYSrr"
      },
      "source": [
        "Exploring features of NLP\n",
        " - Installations\n",
        " - Named Entities\n",
        " - Dependancy Parse & coreference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu39hio9YnVa"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1mixVGGX_n0"
      },
      "outputs": [],
      "source": [
        "!pip install stanfordnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G4ypMnFZLr6"
      },
      "outputs": [],
      "source": [
        "import stanfordnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1-0_hb7bzca"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1iHz7FIb5Wh"
      },
      "outputs": [],
      "source": [
        "!unzip stanford-corenlp-full-2018-10-05.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVdzznW-cLLH"
      },
      "outputs": [],
      "source": [
        "!java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3b_wLPkSW1f"
      },
      "outputs": [],
      "source": [
        "!java /content/stanford-corenlp-full-2018-10-05/edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YY6MxH6cZv6"
      },
      "outputs": [],
      "source": [
        "!export CORENLP_HOME=stanford-corenlp-full-2018-10-05/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo-6O5Z7cYTA"
      },
      "outputs": [],
      "source": [
        "from stanfordnlp.server import CoreNLPClient\n",
        "# example text\n",
        "print('---')\n",
        "print('input text')\n",
        "print('')\n",
        "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
        "print(text)\n",
        "# set up the client\n",
        "print('---')\n",
        "print('starting up Java Stanford CoreNLP Server...')\n",
        "# set up the client\n",
        "with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','depparse','coref'], timeout=30000, memory='16G') as client:\n",
        "    # submit the request to the server\n",
        "    ann = client.annotate(text)\n",
        "    # get the first sentence\n",
        "    sentence = ann.sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QokfBM2ZOlq"
      },
      "outputs": [],
      "source": [
        "stanfordnlp.download('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxGjPEwlZRpd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "\n",
        "\"\"\"\n",
        "Named Entity tagging in Python with NLTK and the Stanford NER tagger\n",
        "\"\"\"\n",
        "\n",
        "PATH_TO_JAR='http://www.java2s.com/Code/Jar/s/Downloadstanfordner127sourcesjar.htm'\n",
        "PATH_TO_MODEL = 'http://www.java2s.com/Code/Jar/s/Downloadstanfordsutimemodels135sourcesjar.htm'\n",
        "\n",
        "\n",
        "tagger = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
        "\n",
        "sentence = 'First up in London will be Riccardo Tisci, onetime Givenchy darling, favorite of Kardashian-Jenners everywhere, who returns to the catwalk with men’s and women’s wear after a year and a half away, this time to reimagine Burberry after the departure of Christopher Bailey.'\n",
        "\n",
        "#split the sentence into words\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "tagged = tagger.tag(words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}